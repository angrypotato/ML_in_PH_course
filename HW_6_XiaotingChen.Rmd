---
title: "HW 6"
author: "Xiaoting Chen"
output:
  pdf_document: default
  html_document:
    number_sections: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warnings = FALSE, fig.align = 'center',  eval = TRUE)
```

You can run the following code to prepare the analysis.
```{r, message=FALSE, warning=FALSE }
library(r02pro)     #INSTALL IF NECESSARY
library(tidyverse)  #INSTALL IF NECESSARY
library(MASS)
library(tree)
library(ggplot2)
my_ahp <- ahp %>% dplyr::select(gar_car, liv_area, lot_area, oa_qual, sale_price) %>%
  na.omit() %>%
  mutate(type = factor(ifelse(sale_price > median(sale_price), "Expensive", "Cheap")))
tr_ind <- 1:(nrow(my_ahp)/20)
my_ahp_train <- my_ahp[tr_ind, ]
my_ahp_test <- my_ahp[-tr_ind, ]
```

Suppose we want to build a tree to predict `sale_price` and `type`. Please answer the following questions.


1. First, we fit a deep regression tree to predict `sale_price` using the training data `my_ahp_train`. Note that, here we use `tree.control` to generate such a deep tree. 
```{r}
my_control <- tree.control(nrow(my_ahp_train), minsize = 2, mindev = 0)
fit <- tree(sale_price ~ gar_car + liv_area + oa_qual, 
            control = my_control,
            data = my_ahp_train)
```



a.  Prune the tree to different subtrees with the number of terminals nodes ranging from 2 to 20 with increment 1. For each subtree, compute the training error and prediction error on the test data `my_ahp_test`. Visualize the relationship of the two errors vs. the subtree size. 

```{r Q1.a}
size <- seq(2, 20)
dta <- sapply(size, function(n) {
  new.fit <- prune.tree(fit, best = n)
  tr_error <- mean((predict(new.fit) - my_ahp_train$sale_price)^2)
  te_error <- mean((predict(new.fit, newdata = my_ahp_test) - my_ahp_test$sale_price)^2)
  c(tr_error, te_error)
})
dta.new <- data.frame(tr_error = dta[1, ], te_error = dta[2, ], size = size) %>%
  pivot_longer(cols = c("tr_error", "te_error"), names_to = "type", values_to = "error")

ggplot(data = dta.new, aes(size, error, color = type)) +
  geom_point() +
  geom_line() +
  ggtitle("Training and testing error vs. the subtree size")
```

b. Generate a pruned tree with the number of terminal nodes determined by cross-validation. Then visualize the tree and provide interpretations.  **Note that if many trees have the same cross-validation error, you want to choose the tree with the fewest terminal nodes.**

```{r Q1.b}
set.seed(0)
cv_tree_reg <- cv.tree(fit)
best_size_reg <- min(cv_tree_reg$size[which(cv_tree_reg$dev == min(cv_tree_reg$dev))])
cv_tree_final <- prune.tree(fit, best = best_size_reg)
plot(cv_tree_final)
text(cv_tree_final)
```
Interpretation: \
The pruned tree has predictor oa_qual with threshold 7.5 as root, results in 13 terminal nodes after 12 splits. Take the rightmost leave as an example: if a testing subject has `oa_qual >= 8.5` and `gar_car >= 2.5` and `liv_area >= 1980`, then its predicted sale price is 483.4.



2. Build a classification tree with the number of terminal nodes determined via cross-validation to predict `expensive` using the training data `my_ahp_train`. Then compute the training and test classification error. 

```{r Q2}
fit.2 <- tree(type ~ gar_car + liv_area + oa_qual + lot_area, 
            control = my_control,
            data = my_ahp_train)
set.seed(0)
cv_tree <- cv.tree(fit.2)
best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
cv_tree_final <- prune.tree(fit.2, best = best_size)
plot(cv_tree_final)
text(cv_tree_final)

pred.tr <- predict(cv_tree_final, type = "class")
tr_error <- mean(pred.tr != my_ahp_train$type)
pred.te <- predict(cv_tree_final, newdata = my_ahp_test, type = "class")
te_error <- mean(pred.te != my_ahp_test$type)
data.frame(tr_error, te_error)
```

3. Question 8.4.3 on Page 361 in ISLRv2. 


$$
Gini = \sum_{k = 1}^{K}{\hat{P}_{\text{mk}}(1 - \hat{P}_{\text{mk}})}  
= \hat{P}_{\text{m1}}(1 - \hat{P}_{\text{m1}}) + \hat{P}_{\text{m2}}(1 - \hat{P}_{\text{m2}}) 
= \hat{P}_{\text{m1}}(1 - \hat{P}_{\text{m1}}) + (1 - \hat{P}_{\text{m1}})(1 - (1 - \hat{P}_{\text{m1}})) 
= 2\hat{P}_{\text{m1}}(1 - \hat{P}_{\text{m1}})
$$
$$
entropy = -\sum_{k = 1}^{K}{\hat{P}_{\text{mk}}*log\hat{P}_{\text{mk}}} 
= -\hat{P}_{\text{m1}}*log\hat{P}_{\text{m1}} -\hat{P}_{\text{m2}}*log\hat{P}_{\text{m2}} 
= -\hat{P}_{\text{m1}}*log\hat{P}_{\text{m1}} -(1 - \hat{P}_{\text{m1}})*log(1 - \hat{P}_{\text{m1}}) 
$$
$$
CE = 1 - max(\hat{P}_{\text{mk}}) 
= 1 - max\{\hat{P}_{\text{m1}}, \hat{P}_{\text{m2}}\} 
= 1 - max\{\hat{P}_{\text{m1}}, {1 - \hat{P}_{\text{m1}}}\}
$$


```{r Q3, warning=FALSE}
G = function(x){2 * x * (1 - x)}
D = function(x){- x * log(x) - (1 - x) * log(1 - x)}
E = function(x){1 - pmax(x, 1-x)}

colors <- c("Gini G" = "red", "entropy D" = "blue", "Classification error" = "orange")
ggplot(data.frame(x=c(0, 1))) + 
  stat_function(fun=G, aes(x=x, color = "Gini G")) +
  stat_function(fun=D, aes(x=x, color = "entropy D")) +
  stat_function(fun=E, aes(x=x, color = "Classification error")) +
  theme_minimal() +
  labs(x = "pm1", y = "value", color = "Legend") +
    scale_color_manual(values = colors)
```

